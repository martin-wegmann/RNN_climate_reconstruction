{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from matplotlib import cm\n",
    "import cartopy.crs as ccrs\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, GRU, Reshape \n",
    "from keras import initializers\n",
    "import matplotlib.pyplot as plt \n",
    "from keras.models import Sequential \n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow.keras as keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from tensorflow.keras import models\n",
    "import os\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-comparison",
   "metadata": {},
   "source": [
    "## Authors\n",
    "* Martin Wegmann (martin.wegmann@giub.unibe.ch)\n",
    "* Fernando Jaume Santero (fernando.jaume@unige.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-dover",
   "metadata": {},
   "source": [
    "### Read in Pseudo-Location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"/Volumes/SPARK/ISTI/EKF400_v1_assim_ISTI_less1831_smallest.txt\", delimiter = \" \")\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlats=len(lats)\n",
    "nlons=len(lons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-islam",
   "metadata": {},
   "source": [
    "### Read in Gridded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathTo20CR = '/Volumes/SPARK/20crv3/'\n",
    "pathToEKF = '/Volumes/SPARK/ekf400v2/ensmean/' \n",
    "save_folder=\"/Volumes/SPARK/RNN_savestates/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = xr.open_dataset(pathTo20CR  + 'air.2m.mon.mean_18512015_anoms_remap.nc')\n",
    "ds1_var=ds1.air\n",
    "ds2 = xr.open_dataset(pathToEKF + 'EKF400_ensmean_v2.0_t2m_anoms.nc')\n",
    "ds2_var=ds2.air_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_dim=ds1_var.shape[2]\n",
    "lat_dim=ds1_var.shape[1]\n",
    "print(lon_dim)\n",
    "print(lat_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitudes=ds1_var.lat.values\n",
    "longitudes=ds1_var.lon.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-strip",
   "metadata": {},
   "source": [
    "### Convert latitude and longitude data of the stations to fit the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_lon=360/lon_dim\n",
    "print(resolution_lon)\n",
    "resolution_lat=180/lat_dim\n",
    "print(resolution_lat)\n",
    "lats=df2.Lat.values\n",
    "lons=df2.Lon.values\n",
    "#lons=lons+lon_dim/2\n",
    "#lats=lats-lat_dim/2\n",
    "lons=(lons+180)/resolution_lon\n",
    "lats=(lats-90)/(resolution_lat*-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of timesteps we have in the training data\n",
    "timesteps_in_data=len(ds1_var.time.values)\n",
    "timesteps_in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of timesteps we want to reconstruct\n",
    "timesteps_in_testdata=len(ds2_var.time.values)\n",
    "timesteps_in_testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-remark",
   "metadata": {},
   "source": [
    "### Set training sample size and amount of channels we want to train with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=timesteps_in_data-1 \n",
    "sample_size=int(sample_size)\n",
    "amount_locations=len(lats)\n",
    "amount_channels=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-incidence",
   "metadata": {},
   "source": [
    "### Set output location and file name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"20cr\"\n",
    "member=\"det\"\n",
    "amount_locations=amount_locations\n",
    "sample_size=sample_size\n",
    "resolution=\"lowres\"\n",
    "output=\"anoms\"\n",
    "\n",
    "\n",
    "\n",
    "RNN1_path=os.path.join(save_folder, \"best_model_50p\" + output + \"_\" + model + \"_\" + resolution +\"_\" +str(amount_locations) + \"_\" + str(sample_size) + \"_RNN1_\"+member+\".h5\")\n",
    "RNN1lstm_path=os.path.join(save_folder, \"best_model_50p\" + output + \"_\" + model + \"_\" + resolution +\"_\" +str(amount_locations) + \"_\" + str(sample_size) + \"_RNN1lstm_\"+member+\".h5\")\n",
    "\n",
    "RNN1_path_nc=os.path.join(save_folder, \"best_model_ekf400_50p_\" + output + \"_\" + model + \"_\" + resolution +\"_\" +str(amount_locations) + \"_\" + str(sample_size) + \"_RNN1_\"+member+\".nc\")\n",
    "RNN1lstm_path_nc=os.path.join(save_folder, \"best_model_ekf400_50p_\" + output + \"_\" + model + \"_\" + resolution +\"_\" +str(amount_locations) + \"_\" + str(sample_size) + \"_RNN1lstm_\"+member+\".nc\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-corrections",
   "metadata": {},
   "source": [
    "### Lets have a look at the gridded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-monaco",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = plt.axes(projection=ccrs.Mollweide(central_longitude=0, globe=None)) \n",
    "tplot=ds1_var.isel(time=0).plot.contourf(ax=ax,\n",
    "levels = 17, transform=ccrs.PlateCarree(), cmap=cm.seismic, cbar_kwargs={'orientation':'vertical',\n",
    "'fraction':0.012, 'pad':0.015, 'aspect':35})\n",
    "\n",
    "tplot.colorbar.set_label('Temperature at 2 meters', size=16) \n",
    "tplot.ylabel_style = {'size':16}\n",
    "ax.set_global()\n",
    "ax.coastlines();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = plt.axes(projection=ccrs.Mollweide(central_longitude=0, globe=None)) \n",
    "tplot=ds2_var.isel(time=0).plot.contourf(ax=ax,\n",
    "levels = 17, transform=ccrs.PlateCarree(), cmap=cm.seismic, cbar_kwargs={'orientation':'vertical',\n",
    "'fraction':0.012, 'pad':0.015, 'aspect':35})\n",
    "\n",
    "tplot.colorbar.set_label('Temperature at 2 meters', size=16) \n",
    "tplot.ylabel_style = {'size':16}\n",
    "ax.set_global()\n",
    "ax.coastlines();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-questionnaire",
   "metadata": {},
   "source": [
    "### Define our Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "ess = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)\n",
    "\n",
    "mc_RNN1 = ModelCheckpoint(RNN1_path, monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "mc_RNN1lstm = ModelCheckpoint(RNN1lstm_path, monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-window",
   "metadata": {},
   "source": [
    "### Define our RNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "N1= Sequential()\n",
    "N1.add(SimpleRNN(50, input_shape=(amount_channels, amount_locations), activation='tanh', unroll=True))\n",
    "#N1.add(Dense(256*512, activation='linear', bias_initializer=initializers. 􏰀→Constant(value=273.15)))\n",
    "N1.add(Dense(lat_dim*lon_dim, activation='linear')) \n",
    "N1.add(Reshape((lat_dim,lon_dim)))\n",
    "N1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "N1_lstm= Sequential()\n",
    "N1_lstm.add(LSTM(50, input_shape=(amount_channels, amount_locations), activation='tanh', unroll=True))\n",
    "#N1.add(Dense(256*512, activation='linear', bias_initializer=initializers. 􏰀→Constant(value=273.15)))\n",
    "N1_lstm.add(Dense(lat_dim*lon_dim, activation='linear')) \n",
    "N1_lstm.add(Reshape((lat_dim,lon_dim)))\n",
    "N1_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-dancing",
   "metadata": {},
   "source": [
    "### Compile our RNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.0001, decay=1e-5) \n",
    "N1.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "N1_lstm.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-layout",
   "metadata": {},
   "source": [
    "### Create our latitude and longitude channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_points = np.random.randint(ds1_var.shape[2], size=(sample_size,1,amount_locations))\n",
    "# _test is the full EKF400v2 time frame we want to reconstruct\n",
    "lon_points_test = np.random.randint(ds1_var.shape[2], size=(timesteps_in_testdata,1,amount_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_points = np.random.randint(ds1_var.shape[1], size=(sample_size,1,amount_locations))\n",
    "lat_points_test = np.random.randint(ds1_var.shape[1], size=(timesteps_in_testdata,1,amount_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_points=lons\n",
    "lat_points=lats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_points=np.array(lat_points,dtype=int)\n",
    "lon_points=np.array(lon_points,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_locations_rand = np.random.rand(sample_size,1,nlons) # 49 for 7*7 locations\n",
    "geo_locations_zero = np.zeros_like(geo_locations_rand, dtype=np.float32)\n",
    "\n",
    "geo_locations_rand_test = np.random.rand(timesteps_in_testdata,1,nlons) # 49 for 7*7 locations\n",
    "geo_locations_zero_test = np.zeros_like(geo_locations_rand_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,timesteps_in_testdata):\n",
    "    geo_locations_zero_test[i,0,:]=lon_points\n",
    "lon_points_test=geo_locations_zero_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,sample_size):\n",
    "    geo_locations_zero[i,0,:]=lon_points\n",
    "lon_points=geo_locations_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_locations_rand = np.random.rand(sample_size,1,nlats) # 49 for 7*7 locations\n",
    "geo_locations_zero = np.zeros_like(geo_locations_rand, dtype=np.float32)\n",
    "\n",
    "geo_locations_rand_test = np.random.rand(timesteps_in_testdata,1,nlats) # 49 for 7*7 locations\n",
    "geo_locations_zero_test = np.zeros_like(geo_locations_rand_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,timesteps_in_testdata):\n",
    "    geo_locations_zero_test[i,0,:]=lat_points\n",
    "lat_points_test=geo_locations_zero_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,sample_size):\n",
    "    geo_locations_zero[i,0,:]=lat_points\n",
    "lat_points=geo_locations_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-ordinary",
   "metadata": {},
   "source": [
    "### create the time domain we want to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = np.random.randint(ds1_var.shape[0]-1, size=(sample_size,1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timesteps_test=list(range(0,timesteps_in_testdata))\n",
    "timesteps_in_testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-header",
   "metadata": {},
   "source": [
    "### sample our data according to the time domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_array = ds1_var[timesteps.flatten(),:,:] # selecting the time steps in the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_array_nonrandom = ds2_var[all_timesteps_test,:,:] # selecting the time steps in the grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_matrix = y1_array.values.reshape((len(timesteps),ds1_var.shape[2]*ds1_var.shape[1])) # reshape to matrix, timesteps, lon*lat\n",
    "y2_matrix = y2_array_nonrandom.values.reshape((len(all_timesteps_test),ds2_var.shape[2]*ds2_var.shape[1])) # reshape to matrix, timesteps, lon*lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.zeros_like(lon_points, dtype=np.float32) # create zero matrix with structure like lon_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.zeros_like(lon_points_test, dtype=np.float32) # create zero matrix with structure like lon_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_points=np.array(lat_points,dtype=int)\n",
    "lon_points=np.array(lon_points,dtype=int)\n",
    "\n",
    "lat_points_test=np.array(lat_points_test,dtype=int)\n",
    "lon_points_test=np.array(lon_points_test,dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-success",
   "metadata": {},
   "source": [
    "### sample our data according to the space domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(timesteps)): \n",
    "        X1[i,0,:]=y1_matrix[i,lat_points[i,0,:]*lon_points[i,0,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_timesteps_test)): \n",
    "        X2[i,0,:]=y2_matrix[i,lat_points_test[i,0,:]*lon_points_test[i,0,:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-target",
   "metadata": {},
   "source": [
    "### normalize our data with the maximum values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_points_normmax=lon_points/lon_dim\n",
    "\n",
    "lat_points_normmax=lat_points/lat_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_points_test_normmax=lon_points_test/lon_dim\n",
    "\n",
    "lat_points_test_normmax=lat_points_test/lat_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_normmax=X1/X1.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_normmax=X2/X1.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_array_normmax=y1_array/y1_array.max()\n",
    "\n",
    "y2_array_nonrandom_normax=y2_array_nonrandom/y1_array.max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-gabriel",
   "metadata": {},
   "source": [
    "### concatenate our X or input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nn = np.concatenate((lat_points_normmax,lon_points_normmax,X1_normmax),axis=1)\n",
    "# input_nn_test has a nonrandom time domain, so the same time structure as EKF400v2\n",
    "input_nn_test = np.concatenate((lat_points_test_normmax,lon_points_test_normmax,X2_normmax),axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-conspiracy",
   "metadata": {},
   "source": [
    "### define our Y or output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values=y1_array_normmax.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-council",
   "metadata": {},
   "source": [
    "### train our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "N1.fit(input_nn, y_values, batch_size=128, epochs=1000, verbose=1,validation_split=0.2, callbacks=[ess, mc_RNN1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "N1_lstm.fit(input_nn, y_values, batch_size=128, epochs=1000, verbose=1,validation_split=0.2, callbacks=[ess, mc_RNN1lstm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "Est_RNN1= N1.predict(input_nn)\n",
    "Est_RNN1_test= N1.predict(input_nn_test)\n",
    "Est_RNN1_test_lstm=N1_lstm.predict(input_nn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalize=y1_array.max().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-boxing",
   "metadata": {},
   "source": [
    "### lets have a look at our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 7))\n",
    "plt.imshow(y1_array_normmax[0,:,:]*y1_array.max()+modulator, vmin=-max, vmax=max, cmap='seismic',origin='upper',interpolation=\"none\") \n",
    "plt.plot(input_nn[half_data,1,:]*lon_dim,input_nn[half_data,0,:]*lat_dim,'|k', markersize=7)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-relations",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 7))\n",
    "plt.imshow(Est_RNN1[0,:,:]*unnormalize+modulator, vmin=-max, vmax=max, cmap='seismic',origin='upper',interpolation=\"none\") \n",
    "plt.plot(input_nn[half_data,1,:]*lon_dim,input_nn[half_data,0,:]*lat_dim,'|k', markersize=7)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-screening",
   "metadata": {},
   "source": [
    "### write our reconstructed fields out as netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "import numpy as np\n",
    "import datetime\n",
    "from netCDF4 import Dataset,num2date,date2num\n",
    "# -----------------------\n",
    "nyears = Est_RNN1_test.shape[0];\n",
    "output=RNN1_path_nc\n",
    "unout = 'days since 1900-01-01 00:00:00'\n",
    "# -----------------------\n",
    "ny, nx = (lat_dim, lon_dim)\n",
    "lon = longitudes\n",
    "lat = latitudes\n",
    "\n",
    "dataout = Est_RNN1_test[:,:,:]*unnormalize; # create some random data\n",
    "datesout = [datetime.datetime(1900+iyear,1,1) for iyear in range(nyears)]; # create datevalues\n",
    "# =========================\n",
    "ncout = Dataset(output,'w','NETCDF4'); # using netCDF3 for output format \n",
    "ncout.createDimension('lon',nx);\n",
    "ncout.createDimension('lat',ny);\n",
    "ncout.createDimension('time',nyears);\n",
    "lonvar = ncout.createVariable('lon','float32',('lon'));lonvar[:] = lon;\n",
    "latvar = ncout.createVariable('lat','float32',('lat'));latvar[:] = lat;\n",
    "timevar = ncout.createVariable('time','float64',('time'));timevar.setncattr('units',unout);timevar[:]=date2num(datesout,unout);\n",
    "myvar = ncout.createVariable(\"t2m\",'float32',('time','lat','lon'));myvar.setncattr('units',\"K\");myvar[:] = dataout;\n",
    "ncout.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "import numpy as np\n",
    "import datetime\n",
    "from netCDF4 import Dataset,num2date,date2num\n",
    "# -----------------------\n",
    "nyears = Est_RNN1_test_lstm.shape[0];\n",
    "output=RNN1lstm_path_nc\n",
    "unout = 'days since 1900-01-01 00:00:00'\n",
    "# -----------------------\n",
    "ny, nx = (lat_dim, lon_dim)\n",
    "lon = longitudes\n",
    "lat = latitudes\n",
    "\n",
    "dataout = Est_RNN1_test_lstm[:,:,:]*unnormalize; # create some random data\n",
    "datesout = [datetime.datetime(1900+iyear,1,1) for iyear in range(nyears)]; # create datevalues\n",
    "# =========================\n",
    "ncout = Dataset(output,'w','NETCDF4'); # using netCDF3 for output format \n",
    "ncout.createDimension('lon',nx);\n",
    "ncout.createDimension('lat',ny);\n",
    "ncout.createDimension('time',nyears);\n",
    "lonvar = ncout.createVariable('lon','float32',('lon'));lonvar[:] = lon;\n",
    "latvar = ncout.createVariable('lat','float32',('lat'));latvar[:] = lat;\n",
    "timevar = ncout.createVariable('time','float64',('time'));timevar.setncattr('units',unout);timevar[:]=date2num(datesout,unout);\n",
    "myvar = ncout.createVariable(\"t2m\",'float32',('time','lat','lon'));myvar.setncattr('units',\"K\");myvar[:] = dataout;\n",
    "ncout.close();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Weatherbench)",
   "language": "python",
   "name": "weatherbench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
